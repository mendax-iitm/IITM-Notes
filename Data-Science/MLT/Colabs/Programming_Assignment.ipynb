{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mendax-iitm/IITM-Notes/blob/main/Data-Science/MLT/Colabs/Programming_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week-5"
      ],
      "metadata": {
        "id": "dIVJEt6Bfeqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1: \n",
        "Define a function cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) having the \n",
        "following characteristics: \n",
        "\n",
        "Input:\n",
        "y: Actual output label vector\n",
        "\n",
        "sigmoid_vector: logistic value of predicted output \n",
        "\n",
        "w: weight vector\n",
        "\n",
        "reg_type: type of regularization as string, either 'l1' or 'l2'. Default 'l2'.\n",
        "\n",
        "reg_rate: regularization rate. Default value 0.\n",
        "\n",
        "Output:\n",
        "Binary cross entropy loss(float value)"
      ],
      "metadata": {
        "id": "y7-SrQTafiRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y, sigmoid_vector,w,reg_type,reg_rate):\n",
        "  loss=-1*(np.sum(y*np.log(sigmoid_vector)+(1-y)*np.log(1-sigmoid_vector)))\n",
        "  if reg_type=='l1':\n",
        "    return loss+reg_rate*np.sum(np.abs(w))\n",
        "  else:\n",
        "    return loss+reg_rate*np.dot(np.transpose(w),w)"
      ],
      "metadata": {
        "id": "p8gMPjPoXP4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2:\n",
        "Write a function sigmoid(X) which returns logistic function of X, where X is a numpy array.\n"
      ],
      "metadata": {
        "id": "hMCFfI6WjtKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))"
      ],
      "metadata": {
        "id": "plih8AlfxqF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-1: \n",
        "Assume that we have trained a logistic regression classifier on a dataset and have learned the weight w. Define a function predict_label(X,w) which accepts a feature matrix X of test samples and the weight vector w as arguments, and assigns labels to each of the samples based on the following conditions:\n",
        "\n",
        "If the model's output is greater than or equal to 0.75, assign the predicted label as 1\n",
        "\n",
        "If the model's output is less than or equal to 0.25, assign the predicted label as -1\n",
        "\n",
        "Otherwise, assign the label as 0\n",
        "\n",
        "The function should return the vector of predicted labels. Use the sigmoid activation function while calculating the model's output for all the sample values in the test-set."
      ],
      "metadata": {
        "id": "K6yucrYPx4wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(X,w):\n",
        "  z=X@w\n",
        "  sigmoid_vector=1/(1+np.exp(-z))\n",
        "  predicted_label=np.zeros(X.shape[0])\n",
        "  predicted_label[sigmoid_vector>=0.75]=1\n",
        "  predicted_label[sigmoid_vector<=0.25]=-1\n",
        "  return predicted_label"
      ],
      "metadata": {
        "id": "Z9L3xVWbxyfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPA-2:\n",
        "Define a function gradient(X, y, w, reg_rate) which can be used for optimization of logistic regression model with L2 regularization having the following characteristics:\n",
        "\n",
        "\n",
        "Input:\n",
        "\n",
        "X: Feature matrix for training data.\n",
        "\n",
        "y: Label vector for training data.\n",
        "\n",
        "reg_rate: regularization rate\n",
        "\n",
        "w: weight_vector\n",
        "\n",
        "Output:\n",
        "\n",
        " A vector of gradients."
      ],
      "metadata": {
        "id": "NQkjfTbZ0HZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(X,y,w,reg_rate):\n",
        "  z=X@w\n",
        "  sigmoid_vector=1/(1+np.exp(-z))\n",
        "  return np.transpose(X)@(sigmoid_vector-y)+reg_rate*w"
      ],
      "metadata": {
        "id": "rxVae29hy1Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-3:\n",
        "Define a function update_w(X, y, w, reg_rate, lr) which can be used for optimization of logistic regression model with L2 regularization having following characteristics:\n",
        "\n",
        "Input:\n",
        "\n",
        "X: Feature matrix for training data.\n",
        "\n",
        "y: Label vector for training data.\n",
        "\n",
        "reg_rate: regularization rate\n",
        "\n",
        "w: weight_vector\n",
        "\n",
        "lr: learning rate\n",
        "\n",
        "Output:\n",
        "\n",
        " A vector of updated weights.\n",
        " \n",
        "You need to perform exactly one update over the entire data."
      ],
      "metadata": {
        "id": "XwJfdTFD07aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_w(X,y,w,reg_rate,lr):\n",
        "  z=X@w\n",
        "  sigmoid_vector=1/(1+np.exp(-z))\n",
        "  grad=np.transpose(X)@(sigmoid_vector-y)+reg_rate*w\n",
        "  return (w-lr*grad)"
      ],
      "metadata": {
        "id": "JfTAyr6K1I2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week-6"
      ],
      "metadata": {
        "id": "mdTYjxSs4TSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1\n",
        "Consider a training dataset $D = \\left \\{x^{(i)}, y^{(i)} \\right \\}_{i = 1}^{100}$\n",
        "​\n",
        "  for a binary classification problem, where the feature vector $x = (x_1, x_2)$ is a two-dimensional binary vector, i.e., each feature is binary. The class label y is indexed using 1 and 2. A sample feature matrix and label vector is given below:\n",
        "\n",
        "$$X = \\begin{bmatrix} 1 & 0\\\\ 0 & 0\\\\ 0 & 1\\\\ 1 & 0 \\end{bmatrix}$$, $$y = \\begin{bmatrix} 1\\\\ 2\\\\ 2\\\\ 2 \\end{bmatrix}$$\n",
        "​\n",
        " \n",
        "Assume that the features are conditionally independent given the class labels. Train a Bernoulli Naive-Bayes classifier for this data. Specifically, estimate the following parameter matrix:\n",
        "\n",
        "$$P = \\begin{bmatrix} p_{11} & p_{12}\\\\ p_{21} & p_{22} \\end{bmatrix}$$\n",
        "\n",
        "This matrix is to be understood as follows. For features $x_1\n",
        "​and x_2$:\n",
        "\n",
        "$p_{ij} = P(x_i = 1\\ |\\ y = j)$\n",
        "\n",
        "​\n",
        " , the first index stands for the feature and the second stands for the class-label.\n",
        "\n",
        "Write a function named **`bernoulli_naive_bayes`** that accepts a feature matrix **X** and a label vector **y** as arguments. It should return the parameter matrix **P**. Both the arguments and the return value are of type **np.ndarray**. You can assume that no smoothing is required."
      ],
      "metadata": {
        "id": "_FeX00c5Smk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bernoulli_naive_bayes(X,y):\n",
        "  n_samples,n_features=X.shape\n",
        "  class_count=np.unique(y)\n",
        "  n_classes=len(class_count)\n",
        "  w=np.zeros((n_classes, n_features))\n",
        "\n",
        "  for idx,c in enumerate(class_count):\n",
        "    X_c=X[y==c]\n",
        "    w[idx,:]=np.sum(X_c,axis=0)/X_c.shape[0]\n",
        "\n",
        "  return w.T\n",
        "    \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "NDofhna95IZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2\n",
        "You are given a numerical data matrix x as an np.ndarray (shape $200 \\times 5)$ and a vector of class labels y (size 200) as np.ndarray for a multi-class classification problem. Define a function mean_estimate which calculates the estimated mean of data samples corresponding to the class labels for each feature and returns a dictionary with class labels as keys and estimated mean vectors as values. The $i^{th}$\n",
        "  element of a mean vector corresponds to the $i^{th}$\n",
        "  feature."
      ],
      "metadata": {
        "id": "ZSaMrc20DJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_estimate(X: np.ndarray,  y : np.ndarray):\n",
        "  labels=np.unique(y)\n",
        "  d={}\n",
        "  for c in labels:\n",
        "    X_c=X[y==c]\n",
        "    k=X_c.mean(axis=0)\n",
        "    d[c]=k\n",
        "  return d\n"
      ],
      "metadata": {
        "id": "wruFnfq0DjPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-3\n",
        "Write a function **naive_gaussian_predict** that implements a Gaussian Naive Bayes model on training data, and returns the predicted class labels for the test data. This is a binary classification task (labels are 0 and 1) and the size of X_train, y_train and X_test are fixed to ($800 \\times 2$), (800, )(and (200 $\\times$ 2) respectively. The function signature is as follows:\n",
        "\n",
        "    Arguments:\n",
        "        X_train: train samples, (800, 2), np.ndarray \n",
        "\n",
        "        y_train: train labels,  (800, ), np.ndarray \n",
        "\n",
        "        X_test:  test samples, (200, 2), np.ndarray\n",
        "\n",
        "    Return  \n",
        "        y_pred: test labels, (200, ) np.ndarray\n"
      ],
      "metadata": {
        "id": "nOWHr6Z6GwS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NB(object):\n",
        "\n",
        "  def fit(self, X,y):\n",
        "    n_samples,n_features=X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    n_classes=len(self._classes)\n",
        "\n",
        "    self._mean=np.zeros(n_classes, n_features)\n",
        "    self._var=np.zeros(n_classes, n_features)\n",
        "    self._priors=np.zeros(n_classes)\n",
        "\n",
        "    for idx,c in enumerate(self._classes):\n",
        "      X_c=X[y==c]\n",
        "      self._mean[idx,:] = X_c.mean(axis=0)\n",
        "      self._var[idx,:]=X_c.var(axis=0)\n",
        "      self._priors[idx]=X_c.shape[0]/n_samples\n",
        "  def _pdf(self,class_idx,X):\n",
        "    mean=self._mean[class_idx]\n",
        "    var=np.diag(self._var[class_idx])\n",
        "    z=np.power(2*np.pi,X.shape[0]/2)*np.power(np.linalg.det(var),1/2)\n",
        "    return (1/z)*np.exp(-(1/2)*(X-mean).T@np.linalg.inv(var)@(X-mean))\n",
        "  ##############################################################\n",
        "  def predict(self,X):\n",
        "    self._posterior=np.zeros(X.shape[0],len(self._classes))\n",
        "    for idx,x in enumerate(X):\n",
        "      for c in self._classes:\n",
        "        self._posterior[idx,c]=np.log(self._pdf(c,x))+np.log(self._priors[c])\n",
        "    return np.argmax(self._posterior,axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUIdzWd5HNpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-4\n",
        "For a binary classification problem with class labels (0 and 1), define a function class_scores that accepts the true and predicted labels and returns the following evaluation metrics as a dictionary.\n",
        "\n",
        "(1) Precision\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Accuracy\n",
        "\n",
        "(4) F1 score\n",
        "\n",
        "(5) Misclassification Rate\n",
        "\n",
        "They keys of the dictionary are the names of the metrics, exactly as they are given above. The values are the corresponding measurements expressed as floats. The function should have the following signature:\n",
        "\n",
        "Arguments:  \n",
        "\n",
        "    y_test: true labels, (n, ), np.ndarray \n",
        "\n",
        "    y_pred: predicted labels, (n, ), np.ndarray\n",
        "Return:\n",
        "\n",
        "    metrics: dictionary\n",
        "        key: string, names of the metrics\n",
        "        value: float\n",
        "Both numpy arrays are of size (n, )(n,). Do not use any existing methods/functions to calculate the same. Consider label 1 as the positive class. Note that the misclassification rate is 1 minus the accuracy.\n"
      ],
      "metadata": {
        "id": "_C8v3663TTMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_scores(y_test,y_pred):\n",
        "  tp=np.where((y_test==1)&(y_pred==1),1,0).sum()\n",
        "  tn=np.where((y_test==0)&(y_pred==0),1,0).sum()\n",
        "  fp=np.where((y_test==0)&(y_pred==1),1,0).sum()\n",
        "  fn=np.where((y_test==1)&(y_pred==0),1,0).sum()\n",
        "  \n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "  f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "  misclassification_rate = 1 - accuracy\n",
        "  return {\"precision\": precision, \"recall\": recall, \"accuracy\": accuracy,\n",
        "           \"f1_score\": f1_score, \"misclassification_rate\": misclassification_rate}\n"
      ],
      "metadata": {
        "id": "YbHYjwyUTgAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-1\n",
        "In a multi-class classification setting, consider a numerical feature matrix X as an np.ndarray of shape (n, m) and a vector of class labels y of size (n, )as an `np.ndarray`. Define a function variance_estimate which calculates the estimated variance of data samples corresponding to individual class labels for each feature. The function should return a dictionary with class labels as keys and estimated variance vectors as values. The $i^{th}$ \n",
        "\n",
        "  element of a vector corresponds to the variance of the $i^{th}$\n",
        "  feature."
      ],
      "metadata": {
        "id": "4iQ6Wx4UVbXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_estimate(X,y):\n",
        "  n_samples,n_features=X.shape\n",
        "  n_classes=np.unique(y)\n",
        "  class_count=len(n_classes)\n",
        "  var=np.zeros((class_count,n_features))\n",
        "  for idx,c in enumerate(n_classes):\n",
        "    X_c=X[y==c]\n",
        "    var[idx,:]=X_c.var(axis=0)\n",
        "\n",
        "  return var\n"
      ],
      "metadata": {
        "id": "h0gcsM7sVtbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-2\n",
        "Write a function naive_gmodel_eval that implements a Gaussian Naive Bayes model on training data, predicts class labels for the test data and returns the evaluation scores for the predictions performed on the test data, corresponding to each individual label appearing on y_test. This is a multiclass classification task and the size of X_train, y_train, X_test and y_test are fixed to (1800, 5), (1800, ), (200, 5) and (200, )respectively. There are four classes labeled as 0, 1, 2, 3\n",
        "\n",
        "For each label, the following evaluation metrics have to be generated by treating that label as a positive class and all others as the negative class.\n",
        "\n",
        "(1) Precision\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Accuracy\n",
        "\n",
        "(4) F1 score\n",
        "\n",
        "(5) Misclassification Rate\n",
        "\n",
        "This information has to be stored in a dictionary. They keys of the dictionary are the names of the metrics, exactly as they are given above. The values are the corresponding measurements expressed as floats.\n",
        "\n",
        "Create a parent dictionary named metrics. The keys of this dictionary will be the labels and the values will be the corresponding evaluation dictionaries. So, your function should return metrics, which is essentially a dictionary of dictionaries!\n",
        "The function will have the following signature:\n",
        "\n",
        "    Arguments:\n",
        "        X_train: training samples, (1800, 5), np.ndarray\n",
        "        y_train: training labels, (1800, ), np.ndarray, labels are 0, 1, 2 or 3\n",
        "        X_test:  test samples, (200, 5), np.ndarray\n",
        "        y_test:  test_labels, (200, ), np.ndarray, labels are 0, 1, 2, or 3\n",
        "    Return:\n",
        "        metrics: dict of dicts\n",
        "           key: label, int\n",
        "           value: dict\n",
        "               key: string\n",
        "               value: float"
      ],
      "metadata": {
        "id": "rxL2erhyWfSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGB(object):\n",
        "  def fit(self,X,y):\n",
        "    n_samples,n_features=X.shape\n",
        "    self._classes=np.unique(y)\n",
        "    n_classes=len(self._classes)\n",
        "\n",
        "    self._mean=np.zeros(n_classes,n_features)\n",
        "    self._var=np.zeros(n_classes,n_features)\n",
        "    self._priors = np.zeros(n_classes)\n",
        "    for idx,c in self._classes:\n",
        "      X_c=X[y==c]\n",
        "      self._mean[idx,:]=X_c.mean(axis=0)\n",
        "      self._var[idx,:]=X_c.var(axis=0)\n",
        "      self._priors[c]=X_c.shape[0]/n_samples\n",
        "  def _pdf(self,class_idx,X):\n",
        "    mean = self._mean[class_idx]\n",
        "    var=np.diag(self._var[class_idx])\n",
        "    z=np.power(2*np.pi,X.shape[0]/2)*np.power(np.linalg.det(var),1/2)\n",
        "    return (1/z)*np.exp(-(1/2)*(X-mean).T@(np.linalg.inv(var))@(X-mean))\n",
        "  def predict(self,X):\n",
        "    self._posterior=np.zeros(X.shape[0],len(self._classes))\n",
        "    for idx,x in enumerate(X):\n",
        "      for c in self._classes:\n",
        "        self._posterior[idx,c]=np.log(self._pdf(c,x))+np.log(self._priors[c])\n",
        "    return np.argmax(self._posterior,axis=1)\n",
        "def naive_gmodel_eval(X_train,y_train,X_test,y_test):\n",
        "  naive_gb=NGB()\n",
        "  naive_gb.fit(X_train,y_train)\n",
        "  y_pred=naive_gb.predict(X_test)\n",
        "  tp=np.where((y_test==1)&(y_pred==1),1,0).sum()\n",
        "  tn=np.where((y_test==0)&(y_pred==0),1,0).sum()\n",
        "  fp=np.where((y_test==0)&(y_pred==1),1,0).sum()\n",
        "  fn=np.where((y_test==1)&(y_pred==0),1,0).sum()\n",
        "  precision=tp/(tp+fp)\n",
        "  recall=tp/(tp+fn)\n",
        "  accuracy=(tp+tn)/len(y_test)\n",
        "  misclassification_rate=1-accuracy\n",
        "  f1_score=2*precision*recall/(precision+recall)"
      ],
      "metadata": {
        "id": "Djo-K3qlWy8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week-7"
      ],
      "metadata": {
        "id": "Jt-tpuY2lMiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1\n",
        "Write a function euclid(a,b) to find Euclidean distance between vectors a and b. Both a and b have shape (n,1), where n is number of features/dimensions.\n",
        "\n",
        "Input: Vectors a and b\n",
        "Output: Euclidean distance between vectors a and b"
      ],
      "metadata": {
        "id": "rFuWBWsolW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclide(a,b):\n",
        "  return np.sum((a-b)**2,axis=1)"
      ],
      "metadata": {
        "id": "H4wraD1flasC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2\n",
        "Write a function one_hot(y) which performs one hot encoding on vector y and then outputs a resultant matrix which can be used for softmax regression as output label matrix. y is row matrix with (n,1) shape, where n is number of samples.\n",
        "Example: if y is [8, 6, 3], its one hot encoding will be [[0, 0, 1],[0, 1, 0],[1, 0, 0]].\n",
        "\n",
        "Input: y: A vector of shape (n,1)\n",
        "Output: A output label matrix of suitable shape."
      ],
      "metadata": {
        "id": "f5Y4_jyhlouT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y):\n",
        "  y=y.reshape(-1,1)\n",
        "  u=np.sort(np.unique(y))\n",
        "  one_hot_encoding=(y==u).astype(int)\n",
        "  return one_hot_encoding"
      ],
      "metadata": {
        "id": "TmyCn4CNlvHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(np.unique(np.array([[8],[6],[3]])))"
      ],
      "metadata": {
        "id": "b2rZN6whmPvl",
        "outputId": "6be72dc5-f30e-4944-c2f6-23c883864585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-1\n",
        "Write a function manhattan(a,b) to find Manhattan distance between vectors a and b. Both a and b are vectors with shape (n,1) where n is number of features/dimensions.\n",
        "\n",
        "Input: Vectors a and b\n",
        "\n",
        "Output: Manhattan distance between vectors a and b"
      ],
      "metadata": {
        "id": "jiW1iXM_md8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manhattan(a,b):\n",
        "  return np.sum(np.sum(np.abs(a-b),axis=1))"
      ],
      "metadata": {
        "id": "MQ4nDKhmm85N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-2\n",
        "Write a function softmax(Z) to find softmax of linear combination of feature matrix and weight vector.\n",
        "Take care of numerical stability as well.\n",
        "\n",
        "Input: Z = X@W, where X is feature matrix of shape (n,m), W is weight matrix (m,k), where n = number of rows, m = number of features and k = number of labels.\n",
        "\n",
        "Output: A matrix of (n,1) shape. Each row corresponds to the label of that row. Each label will have value from 0 to k-1."
      ],
      "metadata": {
        "id": "919966H8nOX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "  exp=np.exp(Z-np.max(Z))\n",
        "  for i in range(len(Z)):\n",
        "    exp[i]/=np.sum(exp[i])"
      ],
      "metadata": {
        "id": "o75JbbxZnVMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-3\n",
        "Write a function knn(class1, class2, x_new) to find in which cluster x_new belongs using 3-NN. Assume cluster 1 and cluster 2 has 5 points each. Use Euclidian distance as distance measure.\n",
        "\n",
        "Input: Two numpy arrays class1 and class2 of shape (5,2) each and a numpy array x_new of shape (2,1)\n",
        "\n",
        "Output: Return class id to which x_new belongs (i.e. 1 or 2) ."
      ],
      "metadata": {
        "id": "UsddUzIPoSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EuclideanDistance(x1,x2):\n",
        "  dist = np.sum((x1-x2)**2,axis=1)\n",
        "  return dist\n",
        "def knn(class1,class2,x_new):\n",
        "  #Your code\n",
        "  X= np.concatenate((class1,class2),axis=0)\n",
        "  y=np.array([1,1,1,1,1,2,2,2,2,2])\n",
        "  distance_vector = EuclideanDistance(X,x_new.reshape(1,2))\n",
        "  k_nearest_neigbours_indices = np.argpartition(distance_vector,3)[:3]\n",
        "  k_nearest_neigbours = y[k_nearest_neigbours_indices]\n",
        "  if np.count_nonzero(k_nearest_neigbours == 2)>np.count_nonzero(k_nearest_neigbours == 1):\n",
        "      return 2\n",
        "  else:\n",
        "      return 1\n"
      ],
      "metadata": {
        "id": "dfs8GsQnoXhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week-8"
      ],
      "metadata": {
        "id": "YzD2AggNZvJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1\n",
        "Write a function named \"hinge_loss\" that computes the hinge loss value for corresponding elements of two vectors namely \n",
        "y_test (5 x 1) and y_pred (5 x 1) and returns the mean of the loss values computed.\n",
        "\n",
        "Note: y_test is the true test label and y_pred is the the predicted label.\n"
      ],
      "metadata": {
        "id": "2Aqb7Vwyplj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hinge_loss(y_pred,y_test):\n",
        "  return np.maximum(1-y_pred*y_test,0)\n"
      ],
      "metadata": {
        "id": "Gvexv9oUZ3XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2\n",
        "Write a function **'solve_eqn'** to obtain the weight vector (bias as its last element) of linear SVM model by accepting \n",
        "an array of support vectors A of shape (3 x 2) and their label vector b of shape (3 x 1). \n",
        "This function should return weight vector of shape 3 x 1."
      ],
      "metadata": {
        "id": "fYt8zfZKaVR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_eqn(A,b):\n",
        "  X=np.column_stack((A,np.ones(A.shape[0])))\n",
        "  w=np.linalg.solve(X,b)\n",
        "  return w\n"
      ],
      "metadata": {
        "id": "_9DJberRajJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-3:\n",
        "Write a class named 'fit_softsvm' that implements soft margin SVM using GD. \n",
        "\n",
        "Write a separate function named 'support_vectors' and return the support_vectors identified by the model. \n",
        "\n",
        "The inputs to the 'support_vectors' function should be feature matrix X_train and label vector y_train.\n",
        "\n",
        "Use the following parameters:\n",
        "\n",
        "1.   learning rate=0.001\n",
        "2.   C=500\n",
        "3.   epochs=100"
      ],
      "metadata": {
        "id": "dCZjYSiSe1p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fit_softSVM:\n",
        "  def __init__(self,C=500):\n",
        "    self.C=C\n",
        "    self.w=None\n",
        "    self.b=None\n",
        "  def __decision_function(self,X):\n",
        "    return X@self.w+self.b\n",
        "\n",
        "  def __cost(self,margin):\n",
        "    return (1/2)*self.w@self.w+self.C*np.sum(np.maximum(1-margin,0))\n",
        "  def __margin(self,X,y):\n",
        "    return y*self._decision_function(X)\n",
        "  def fit(self,X,y,lr=0.001,epochs=100):\n",
        "    n,d=X.shape\n",
        "    self.w=np.random.randn(d)\n",
        "    self.b=0\n",
        "    ###################################################\n",
        "    for i in range(epochs):                           #\n",
        "      margin=self.__margin(X,y)                       #\n",
        "      miss_idx = np.where(margin<1)[0]                #\n",
        "      d_w=self.w-self.C*y[miss_idx]@X[miss_idx]       #\n",
        "      self.w=self.w-lr*d_w                            #\n",
        "      d_b=-self.C*np.sum(y[miss_idx])                 #\n",
        "    support_vectors=np.where(self.__margin(X,y)<=1)[0]#\n",
        "    return support_vectors                            #\n",
        "  ########################################################"
      ],
      "metadata": {
        "id": "lTyBS9kifkjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week-9\n"
      ],
      "metadata": {
        "id": "FqIjZ6j2n0E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overall_entropy(df):\n",
        "  target=df.keys()[-1]\n",
        "  values_in_target=df[target].unique()\n",
        "  overall_entropy=0\n",
        "  for value in values_in_target:\n",
        "    p=df[target].value_counts()[value]/len(df[target])\n",
        "    overall_entropy+=-p*np.log2(p)\n",
        "  return overall_entropy"
      ],
      "metadata": {
        "id": "GjBun0gUn4y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps=np.finfo(float).eps"
      ],
      "metadata": {
        "id": "0ePafZXLt7sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_entropy_of_attribute(df,attribute):\n",
        "  target=df.keys()[-1]\n",
        "  values_in_target=df[target].unique()\n",
        "  values_in_attribute=df[attribute].unique()\n",
        "  entropy_attribute=0\n",
        "  for value_in_attribute in values_in_attribute:\n",
        "    overall_entropy=0\n",
        "    for value_in_target in values_in_target:\n",
        "      num=len(df[attribute][df[attribute]==value_in_attribute][df[target]==value_in_target])\n",
        "      den=len(df[attribute][df[attribute]==value_in_attribute])\n",
        "      p=num/(den+eps)\n",
        "      overall_entropy+=-p*np.log2(p+eps)\n",
        "    p2=den/len(df)\n",
        "    entropy_attribute+=-p2*overall_entropy\n",
        "  return np.abs(entropy_attribute)\n",
        "\n"
      ],
      "metadata": {
        "id": "_DNk_6CcofNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_attribute_to_divide(df):\n",
        "  #informatio gain\n",
        "  IG=[]\n",
        "\n",
        "  #all column names\n",
        "  all_attribute_names=df.keys()[:-1]\n",
        "\n",
        "  for attribute in all_attribute_names:\n",
        "    #compute IG for every attribute\n",
        "    IG.append(overall_entropy(df)-find_entropy_of_attribute(df,attribute))\n",
        "\n",
        "  #get the index of attribute with best IG\n",
        "  index_of_attribute_with_max_IG=np.argmax(IG)\n",
        "\n",
        "  #print(index of attribute with max IG)\n",
        "  best_attribute = all_attribute_names[index_of_attribute_with_max_IG]\n",
        "  return best_attribute"
      ],
      "metadata": {
        "id": "sIIMitfXxJsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_qzOS1SOwns3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([['overcast', 'hot', 'high', 'FALSE', 'yes'],\n",
        "['overcast', 'cool', 'normal', 'TRUE', 'yes'],\n",
        "['overcast', 'mild', 'high', 'TRUE', 'yes'],\n",
        "['overcast', 'hot', 'normal', 'FALSE', 'yes'],\n",
        "['rainy', 'mild', 'high', 'FALSE', 'yes'],\n",
        "['rainy', 'cool', 'normal', 'FALSE', 'yes'],\n",
        "['rainy', 'cool', 'normal', 'TRUE', 'no'],\n",
        "['rainy', 'mild', 'normal', 'FALSE', 'yes'],\n",
        "['rainy', 'mild', 'high', 'TRUE', 'no'],\n",
        "['sunny', 'hot', 'high', 'FALSE', 'no'],\n",
        "['sunny', 'hot', 'high', 'TRUE', 'no'],\n",
        "['sunny', 'mild', 'high', 'FALSE', 'no'],\n",
        "['sunny', 'cool', 'normal', 'FALSE', 'yes'],\n",
        "['sunny', 'mild', 'normal', 'TRUE', 'yes']], columns=['outlook','temp','humidity','windy','play'])"
      ],
      "metadata": {
        "id": "ilNBH5r-wjn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(df['play'],return_counts=True)"
      ],
      "metadata": {
        "id": "GA4N0xgNwpkR",
        "outputId": "e5c5f62a-0554-46c7-cb36-e10e30aaf892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['no', 'yes'], dtype=object), array([5, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def buildTree(df,tree=None):\n",
        "  target=df.keys()[-1]\n",
        "  node=find_best_attribute_to_divide(df)\n",
        "  attValues=np.unique(df[node])\n",
        "  if tree is None:\n",
        "    tree={}\n",
        "    tree[node]={}\n",
        "\n",
        "  for value in attValues:\n",
        "    subtable=df[df[node]==value].reset_index(drop=True)\n",
        "    c1Value,counts=np.unique(subtable[target],return_counts=True)\n",
        "\n",
        "    if len(counts)==1:\n",
        "      tree[node][value]=c1Value[0]\n",
        "    else:\n",
        "      tree[node][value]=buildTree(subtable)\n",
        "  return tree\n",
        "\n"
      ],
      "metadata": {
        "id": "Gr6Yc-ebwyw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1\n",
        "Consider a regression problem with feature matrix X with size (100, 10) and label vector y with size (100, 1). We split this root node into two nodes 'node1' and 'node2' according to jth split variable and split value 's'.\n",
        "Define a function `predict_node(X, y, j, s)` that takes X, y, split variable j and split value s as parameters and returns the tuple of predict values (mean value) at both the nodes.\n",
        "\n",
        "X = ndarray of size (100, 10) with entries as float.\n",
        "\n",
        "y = ndarray of size (100, 1) with entries as float.\n",
        "\n",
        "j = int\n",
        "\n",
        "s = float"
      ],
      "metadata": {
        "id": "dauVhd6SzCUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_node(X,y,j,s):\n",
        "  node1,node2,n1,n2=0,0,0,0\n",
        "  for i in range(len(X)):\n",
        "    if X[i][j]<=s:\n",
        "      node1+=y[i][0]\n",
        "      n1+=1\n",
        "    else:\n",
        "      node2+=y[i][0]\n",
        "      n2+=1\n",
        "  return (node1/n1,node2/n2)"
      ],
      "metadata": {
        "id": "q4l228mcybxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2\n",
        "Define a function `predict_class(y)` that takes the parameter\n",
        "`y` which is a ndarray of actual outputs of all the samples in a particualar node and retruns the predict class for the same node.\n",
        "\n",
        "Note: number of classes are 10 (0 to 9). \n",
        "\n",
        "If two classes have same number of samples, function should return the lower class. "
      ],
      "metadata": {
        "id": "Kg0_bEeo0myE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(y):\n",
        "  return np.bincount(y).argmax()"
      ],
      "metadata": {
        "id": "WwpAgK4q0q8K",
        "outputId": "a0847c8b-8cb2-4265-afbe-b38ff9c7375c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-3\n",
        "Define a function `misclassification_error(y)`  that takes the parameter `y` which is a ndarray of actual outputs of all the samples in a particular node and return the misclassification error of the same node.\n"
      ],
      "metadata": {
        "id": "XDY_nwpv2Kud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def misclassification_error(y):\n",
        "  max_freq=max(np.bincount(y))\n",
        "  return 1-(max_freq/len(y))"
      ],
      "metadata": {
        "id": "NisDeMXq1ekG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-1\n",
        "Define a function `gini_index(dict)` having the \n",
        "following characteristics:\n",
        "\n",
        "Input:\n",
        "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
        "\n",
        "Output:\n",
        "Gini index of the same node. (A float value)"
      ],
      "metadata": {
        "id": "HxuMC1N73PNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index(dict):\n",
        "  n=sum(dict.values())\n",
        "  p_sum=0\n",
        "  for value in dict.values():\n",
        "    p_sum+=value/n*(1-(value/n))\n",
        "  return p_sum"
      ],
      "metadata": {
        "id": "-fp9BtmR3RNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-2\n",
        "Define a function `entropy(dict)` having the \n",
        "following characteristics:\n",
        "\n",
        "Input:\n",
        "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
        "\n",
        "Output:\n",
        "Entropy of the same node. (A float value)"
      ],
      "metadata": {
        "id": "TIGjJyUS4gJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(dict):\n",
        "  n=sum(dict.values())\n",
        "  entropy=0\n",
        "  for value in dict.values():\n",
        "    entropy+=(value/n)*np.log2(value/n)\n",
        "  return -entropy"
      ],
      "metadata": {
        "id": "FXmYK8n34bY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-3\n",
        "Consider a regression problem using CART. If y is a ndarray of targets of the samples in a particular node, define a function `sseloss(y)` that returns the error associated with that node."
      ],
      "metadata": {
        "id": "DtYFAuXk-HQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sseloss(y):\n",
        "  mean=np.average(y)\n",
        "  loss=np.sum((y-mean)**2)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "EwiOJlqE9vwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week-10\n"
      ],
      "metadata": {
        "id": "WEfF11clIztR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-1\n",
        "Write a function similarity(res,lambda_) which takes residuals at a node as input and returns similarity score which can be used for XGBoost regression.\n",
        "\n",
        "Input: A numpy array having residuals at a node.\n",
        "          lambda is the regularization rate.\n",
        "Output: Similarity score of the node.\n",
        "\n",
        "Note: Similarity Score= (Sum of residuals)2/(No. of residuals+lambda_)"
      ],
      "metadata": {
        "id": "0tzBHr0sLKZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(res,lambda_):\n",
        "  sim=(np.sum(res)**2)/(len(res)+lambda_)\n",
        "  return sim"
      ],
      "metadata": {
        "id": "Gc3CuB4B-668"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPA-2\n",
        "Write a function gradboost(model,X_train, y_train, X_test, boosting_rounds,learning_rate) to implement Gradient boost algorithm.\n",
        "\n",
        "Input:\n",
        "model: model to be fitted\n",
        "X_train: Training features\n",
        "y_train: Training output labels\n",
        "X_test: Test feature values\n",
        "boosting_rounds: number of boosting rounds\n",
        "learning_rate: learning rate used in the algorithm\n",
        "\n",
        "Output:\n",
        "y_hat_train: Prediction on training data after number of boosting rounds\n",
        "y_hat_test:  Prediction on testing data after number of boosting rounds"
      ],
      "metadata": {
        "id": "Q5zdOi2FLpmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradboost(model,X_train,y_train,X_test,boosting_rounds, learning_rate):\n",
        "  y_hat_train=np.repeat(np.mean(y_train),len(y_train))\n",
        "  y_hat_test=np.repeat(np.mean(y_train),len(X_test))\n",
        "  residuals = y_train-y_hat_train\n",
        "  for _ in range(boosting_rounds):\n",
        "    model=model.fit(X_train,residuals)\n",
        "    y_hat_train = y_hat_train+learning_rate*model.predict(X_train)\n",
        "    y_hat_test = y_hat_test+learning_rate*model.predict(X_test)\n",
        "    residuals=y_train-y_hat_train\n",
        "  return y_hat_train, y_hat_test"
      ],
      "metadata": {
        "id": "42A8KaLULf49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPA-1\n",
        "Write a function reidual(y) which takes a numpy array y as input and calculates residuals for base model(taking mean of the target values).\n",
        "\n",
        "Input: numpy array y having all the target values.\n",
        "Output: numpy array containing residuals."
      ],
      "metadata": {
        "id": "cGO7cQc6M_gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual(y):\n",
        "  y_hat= np.repeat(np.mean(y),len(y))\n",
        "  return y-y_hat\n",
        "  "
      ],
      "metadata": {
        "id": "vZJmF36yM9mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag(X,y):\n",
        "  n_samples=X.shape[0]\n",
        "  indices=np.random.choice(n_samples, size=n_samples,replace=True)\n",
        "  return X[indices],y[indices]"
      ],
      "metadata": {
        "id": "W7mxiDDFNTGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def fit(X,k):\n",
        "#   centroids=X[np.random.randint(0,X.shape[0],size=k)]\n",
        "#   labels=np.argmin(cdist(X,centroids),axis=1)\n",
        "\n",
        "#   for _iteration in range(10):\n",
        "#     previous_label=label.copy()\n",
        "#     centroids = np.array([np.mean(X[label==r],axis=0) for r in range(k)])\n",
        "#     labels=np.argmin(cdist(X,centroids),axis=1)\n",
        "\n",
        "#     if all(labels==previous_labels):\n",
        "#       break\n",
        "#     return centroids, labels"
      ],
      "metadata": {
        "id": "VWD8xuvwNvMx",
        "outputId": "7b38f514-6799-4e7d-a6ec-71ef53f048d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-3b9eda616062>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    centroids=\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def polynomial_transform(x,degree):\n",
        "#   if x.ndim==1:\n",
        "#     x=x.reshape(-1,1)\n",
        "#   x_t=x.transpose()\n",
        "#   features=[np.ones(len(x))]\n",
        "#   for degree in range(1,degree+!):\n",
        "#     for items in get_combinations(x_t,degree):\n",
        "#       features.append(compute_new_feature(items))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "23ZCKMMXTmkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([1,2,3,4])\n",
        "[np.ones(3)]"
      ],
      "metadata": {
        "id": "y62mLb9BWl-e",
        "outputId": "05bb3f71-7c52-4e8e-d3ec-1c77e88c23fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1., 1., 1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QdMRs8DkWqCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Programming Assignment",
      "provenance": [],
      "collapsed_sections": [
        "ZSaMrc20DJwK",
        "nOWHr6Z6GwS7",
        "_C8v3663TTMm",
        "4iQ6Wx4UVbXa",
        "rxL2erhyWfSb",
        "rFuWBWsolW1P",
        "f5Y4_jyhlouT",
        "jiW1iXM_md8F",
        "919966H8nOX4",
        "UsddUzIPoSfG",
        "2Aqb7Vwyplj3",
        "fYt8zfZKaVR3",
        "dCZjYSiSe1p2",
        "dauVhd6SzCUU",
        "Kg0_bEeo0myE",
        "XDY_nwpv2Kud",
        "HxuMC1N73PNM",
        "TIGjJyUS4gJ2",
        "DtYFAuXk-HQw",
        "0tzBHr0sLKZJ",
        "Q5zdOi2FLpmA",
        "cGO7cQc6M_gb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}